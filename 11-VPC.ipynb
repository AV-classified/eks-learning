{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "<h2><b> Adnan Rashid </b></h2>\n",
    "    <nav>\n",
    "        <h3>\n",
    "    | <a href=\"00-Contents-Setup.ipynb\"> Home Page </a> | \n",
    "        <a href=\"http://bit.ly/cka_notes_original\"> Kubernetes Notes </a> |\n",
    "        <a href=\"https://adnan.study\"> Website </a> |\n",
    "        <a href=\"https://www.instagram.com/adnans_techie_studies/\"> Instagram </a> |\n",
    "        <a href=\"https://www.linkedin.com/in/adnanrashid1/\"> LinkedIn </a> |\n",
    "        </h3>\n",
    "    </nav>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background:black\"><code style=\"background:black;color:white\"><center>Advanced VPC Networking with EKS</center></code></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<font style=\"color:black\">\n",
    "<center>We will review some of the advanced VPC networking features with EKS </center>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://adnanstudyimages.s3-eu-west-1.amazonaws.com/24.jpg\" width=\"50%\" height=\"50%\" border=5/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br><br> Using Secondary CIDRS with EKS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can expand your VPC network by adding additonal CIDR ranges. \n",
    "* This capability can be used if you are running out of IP ranges within your existing VPC or if you have confusmed all available RFC 1918 CIDR ranges within your coporate network. \n",
    "* EKS supports additional IPv4 CIDR blocks in the 100.64.0.0/10 and 198.19.0.0/16 ranges\n",
    "\n",
    "Below we go through the configuration that is needed so that you can launch your Pod networking on top of secondary CIDRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check your variables are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load in variable store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load variables into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AWS_PROFILE=$AWS_PROFILE\n",
    "%env AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION\n",
    "%env ACCOUNT_ID=$ACCOUNT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check identity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before configuring EKS, we need to enable secondary CIDR blocks in the VPC and make sure they have proper tags and route table configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add secondary CIDRs to your VPC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 describe-vpcs --filters Name=tag:Name,Values=eksctl-eksworkshop-eksctl-cluster/VPC | jq -r '.Vpcs[].VpcId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env VPC_ID=vpc-0a56b9642c5e016e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 associate-vpc-cidr-block --vpc-id $VPC_ID --cidr-block 100.64.0.0/16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next step is to create subnets. Before we do this step, lets check how many subnets we are consuming. \n",
    "* We can run this command to see EC2 instnace and AZ details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 describe-instances --filters \"Name=tag-value,Values=eksworkshop*\"  --query 'Reservations[*].Instances[*].[PrivateDnsName,Tags[?Key==`eks:nodegroup-name`].Value|[0],Placement.AvailabilityZone,PrivateIpAddress,PublicIpAddress]' --output table   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Currently using 3 instances in 3 subnets \n",
    "* We will use the same AZ's and create 3 secondary CIDR subnets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is easier to do as a single block so will comment in the code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# Set the AZ variables \n",
    "export AZ1=eu-west-1a\n",
    "export AZ2=eu-west-1b\n",
    "export AZ3=eu-west-1c\n",
    "\n",
    "# Get the Subnet ID for each for the subnets \n",
    "export CGNAT_SNET1=$(aws ec2 create-subnet --cidr-block 100.64.0.0/19 --vpc-id $VPC_ID --availability-zone $AZ1 | jq -r .Subnet.SubnetId)\n",
    "export CGNAT_SNET2=$(aws ec2 create-subnet --cidr-block 100.64.32.0/19 --vpc-id $VPC_ID --availability-zone $AZ2 | jq -r .Subnet.SubnetId)\n",
    "export CGNAT_SNET3=$(aws ec2 create-subnet --cidr-block 100.64.64.0/19 --vpc-id $VPC_ID --availability-zone $AZ3 | jq -r .Subnet.SubnetId)\n",
    "\n",
    "# We will need this later for cleanup \n",
    "echo \"%env CGNAT_SNET1=$CGNAT_SNET1\"\n",
    "echo \"%env CGNAT_SNET2=$CGNAT_SNET2\"\n",
    "echo \"%env CGNAT_SNET3=$CGNAT_SNET3\"\n",
    "\n",
    "# Create the tags on each of the subnets \n",
    "\n",
    "aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl \n",
    "aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared \n",
    "aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=kubernetes.io/role/elb,Value=1 \n",
    "aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl \n",
    "aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared \n",
    "aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=kubernetes.io/role/elb,Value=1\n",
    "aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl \n",
    "aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared \n",
    "aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=kubernetes.io/role/elb,Value=1 \n",
    "\n",
    "# Now we need to associate the three new subnets into a route table. \n",
    "\n",
    "export SNET1=$(aws ec2 describe-subnets --filters Name=cidr-block,Values=192.168.0.0/19 | jq -r '.Subnets[].SubnetId')\n",
    "export RTASSOC_ID=$(aws ec2 describe-route-tables --filters Name=association.subnet-id,Values=$SNET1 | jq -r '.RouteTables[].RouteTableId')\n",
    "\n",
    "aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET1 \n",
    "aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET2 \n",
    "aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Saving the variables for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CGNAT_SNET1=subnet-0e1f237091c482a0e\n",
    "%env CGNAT_SNET2=subnet-01cbfaea3a30f84ca\n",
    "%env CGNAT_SNET3=subnet-0f44727dd6e52f401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br><br> Configure CNI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before we start making changes to VPC CNI, let's make sure we are using latest CNI version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d \"/\" -f 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* upgrade to latest version if necesssary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.6/aws-k8s-cni.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check status of pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -n kube-system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <br><br>Configure Custom Networking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Edit aws-node configmap and add <b>'AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG'</b> environment variable to the node container spec and set it to true\n",
    "* You only need to set one environment variable in the CNI daemonset configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl set env ds aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe daemonset aws-node -n kube-system | grep -A5 Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Terminate worker nodes so that Autoscaling launches newer nodes that come bootstrapped with custom network config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \"Name=tag-value,Values=eksworkshop-eksctl\" --output text` )\n",
    "for i in \"${INSTANCE_IDS[@]}\"\n",
    "do\n",
    "\techo \"Terminating EC2 instance $i ...\"\n",
    "\taws ec2 terminate-instances --instance-ids $i\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check nodes if they are back, you may need to wait 5-10 minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br><br>Create CRDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom resources for ENIConfig CRD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will add custom resources to ENIConfig custom resource definition (CRD)\n",
    "* CRDs are extensions of Kubernetes API that stores collection of API objects of a certain kind\n",
    "* In this case, we will store VPC Subnet and SecurityGroup configuration information in these CRDs so that Worker nodes can access them to configure VPC CNI plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check ENIConfig CRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get crd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the ENIConfig is not installed, can install with this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.3/aws-k8s-cni.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create custom resources for each subnet by replacing Subnet and SecurityGroup IDs\n",
    "* Since we created three secondary subnets, we need three custom resources \n",
    "\n",
    "Here is the template for custom resource.\n",
    "\n",
    "* Subnet ID and SecurityGroup ID need to be replaced with appropriate values in the CRD folder shortly "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apiVersion: crd.k8s.amazonaws.com/v1alpha1\n",
    "kind: ENIConfig\n",
    "metadata:\n",
    " name: group1-pod-netconfig\n",
    "spec:\n",
    " subnet: $SUBNETID1\n",
    " securityGroups:\n",
    " - $SECURITYGROUPID1\n",
    " - $SECURITYGROUPID2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check the AZs and Subnet IDs for these subnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 describe-subnets  --filters \"Name=cidr-block,Values=100.64.*\" --query 'Subnets[*].[CidrBlock,SubnetId,AvailabilityZone]' --output table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check Worker Node SecurityGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \"Name=tag-value,Values=eksworkshop*\" --output text`)\n",
    "for i in \"${INSTANCE_IDS[@]}\"\n",
    "do\n",
    "  echo \"SecurityGroup for EC2 instance $i ...\"\n",
    "  aws ec2 describe-instances --instance-ids $i  --profile gr-sandbox-admin --region eu-west-1 | jq -r '.Reservations[].Instances[].SecurityGroups[].GroupId'\n",
    "done  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In crd/group1-pod-netconfig.yaml for the first subnet <b>(100.64.0.0/19)</b> replace the SubnetID and SecurityGroupIds with the values from above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat crd/group1-pod-netconfig.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to do similar for the second and third subnet, you can use the same security groups\n",
    "    * CRD/group2-pod-netconfig.yaml for the second subnet <b>(100.64.32.0/19)</b>\n",
    "    * CRD/group3-pod-netconfig.yaml for the third subnet <b> (100.64.64.0/19)</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat crd/group2-pod-netconfig.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat crd/group3-pod-netconfig.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check the instance details using the below as <b>you will need AZ info when you apply annotation to Worker nodes</b> using custom network config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 describe-instances --filters \"Name=tag-value,Values=eksworkshop*\" --query 'Reservations[*].Instances[*].[PrivateDnsName,Tags[?Key==`eks:nodegroup-name`].Value|[0],Placement.AvailabilityZone,PrivateIpAddress,PublicIpAddress]' --output table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply the CRDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f CRD/group1-pod-netconfig.yaml \n",
    "!kubectl apply -f CRD/group2-pod-netconfig.yaml \n",
    "!kubectl apply -f CRD/group3-pod-netconfig.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Annotate nodes with custom network configs \n",
    "* Make sure the subnets align, you will need to check above the subnet the instance and the config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl annotate node ip-192-168-62-249.eu-west-1.compute.internal k8s.amazonaws.com/eniConfig=group1-pod-netconfig\n",
    "!kubectl annotate node ip-192-168-83-85.eu-west-1.compute.internal k8s.amazonaws.com/eniConfig=group2-pod-netconfig\n",
    "!kubectl annotate node ip-192-168-9-192.eu-west-1.compute.internal k8s.amazonaws.com/eniConfig=group3-pod-netconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the coredns pods are not in a ready state, then someone has gone wrong above, check the security groups and the subnets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -n kube-system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br><br>Test Networking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch pods into Secondary CIDR network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's launch few pods and test networking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl scale deployment nginx-deployment --replicas=3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl expose deployment nginx-deployment --type=NodePort --port 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test access to the internet and to nginx service (Will need to run in console as interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:black\"><code style=\"background:black;color:white\">> kubectl run -i --rm --tty debug --image=busybox -- sh</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test access to internet and to nginx service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:black\"><br>\n",
    "<code style=\"background:black;color:white\">> wget google.com -O - <br><br></code>\n",
    "<code style=\"background:black;color:white\">> wget nginx-deployment -O - <br><br></code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br><br> Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete deployments --all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comment out 'AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG' and its value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "...\n",
    "    spec:\n",
    "      containers:\n",
    "      - env:\n",
    "        #- name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG\n",
    "        #  value: \"true\"\n",
    "        - name: AWS_VPC_K8S_CNI_LOGLEVEL\n",
    "          value: DEBUG\n",
    "        - name: MY_NODE_NAME\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:black\">\n",
    "<code style=\"background:black;color:white\">> kubectl edit daemonset -n kube-system aws-node\n",
    "</code>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Delete custom resource objects from ENIConfig CRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete eniconfig/group1-pod-netconfig\n",
    "!kubectl delete eniconfig/group2-pod-netconfig\n",
    "!kubectl delete eniconfig/group3-pod-netconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Terminate EC2 instances so fresh instances are launched with default CNI configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \"Name=tag-value,Values=eksworkshop*\" --output text`)\n",
    "for i in \"${INSTANCE_IDS[@]}\"\n",
    "do\n",
    "\techo \"Terminating EC2 instance $i ...\"\n",
    "\taws ec2 terminate-instances --instance-ids $i \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Delete secondary CIDR from your VPC, you may need to get the values from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=eksctl-eksworkshop* | jq -r '.Vpcs[].VpcId')\n",
    "\n",
    "aws ec2 delete-subnet --subnet-id $CGNAT_SNET1\n",
    "aws ec2 delete-subnet --subnet-id $CGNAT_SNET2\n",
    "aws ec2 delete-subnet --subnet-id $CGNAT_SNET3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get Association ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 describe-vpcs --vpc-id $VPC_ID | jq -r '.Vpcs[].CidrBlockAssociationSet[] | select(.CidrBlock == \"100.64.0.0/16\") | .AssociationId'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Disassociate the secondary CIDR block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 disassociate-vpc-cidr-block --association-id vpc-cidr-assoc-030d456d7bddbc0d0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Configure CNI again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.6/aws-k8s-cni.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
